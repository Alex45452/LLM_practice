{"cells":[{"cell_type":"code","source":["!pip install accelerate\n","!pip install transformers\n","!pip install bitsandbytes\n","!pip install datasets\n","!pip install rouge-score\n","!pip install pymorphy3\n","!pip install peft\n","!pip install unsloth\n","!pip install flash_attn"],"metadata":{"id":"vuzyViB6anP7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6c4b1128-d421-4385-fce4-bd55d0e6f77f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["^C\n","^C\n","^C\n","^C\n"]}]},{"cell_type":"code","source":["!git clone https://huggingface.co/datasets/nyu-mll/glue\n","!git clone https://github.com/RefalMachine/llmtf_open\n","%cd llmtf_open"],"metadata":{"id":"6HSoJBPKbyRN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JZkyfKM5bNHt"},"outputs":[],"source":["import random\n","import codecs\n","import torch\n","import json\n","import re\n","import copy\n","import numpy as np\n","import os\n","from tqdm import tqdm\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForCausalLM,\n","    DataCollatorForTokenClassification,\n","    AutoConfig,\n",")\n","from transformers import (\n","    Trainer,\n","    TrainingArguments,\n","    logging,\n","    TrainerCallback,\n","    TrainerState,\n","    TrainerControl,\n","    BitsAndBytesConfig,\n",")\n","from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n","from unsloth import FastLanguageModel, UnslothTrainingArguments, UnslothTrainer\n","\n","from peft import get_peft_model, LoraConfig\n","from peft import prepare_model_for_kbit_training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IFxKmbf2usFx"},"outputs":[],"source":["from typing import List, Dict\n","\n","from torch.utils.data import Dataset\n","\n","\n","class ChatDataset(Dataset):\n","    def __init__(\n","        self,\n","        original_records: List[Dict],\n","        tokenizer: AutoTokenizer,\n","        max_tokens_count: int,\n","        sample_rate: float = 1.0,\n","        only_target_loss: bool = True,\n","        add_global_bos: bool = True,\n","        add_global_eos: bool = True,\n","        labels_pad_token_id: int = -100\n","    ):\n","        self.original_records = original_records\n","        self.sample_rate = sample_rate\n","        self.tokenizer = tokenizer\n","        self.max_tokens_count = max_tokens_count\n","        self.only_target_loss = only_target_loss\n","        self.labels_pad_token_id = labels_pad_token_id\n","        self.add_global_bos = add_global_bos\n","        self.add_global_eos = add_global_eos\n","        self.is_printed = False\n","\n","        self.records = []\n","        for record in tqdm(original_records):\n","            # if random.random() > self.sample_rate:\n","            #     continue\n","            # не является необходимым в условиях задачи\n","            tensors = self.convert_record(record)\n","            if tensors is None:\n","                continue\n","            self.records.append(tensors)\n","\n","    def __len__(self):\n","        return len(self.records)\n","\n","    def __getitem__(self, index):\n","        return self.records[index]\n","\n","    def get_tokens(self, messages):\n","        #print(messages)\n","        tokens = self.tokenizer.apply_chat_template(\n","            messages,\n","            add_special_tokens=False,\n","            tokenize=True,\n","            add_generation_prompt=False,\n","        )\n","        if tokens[0] == self.tokenizer.bos_token_id:\n","            tokens = tokens[1:]\n","        # почему мы обрезаем токен начала строки?\n","        return tokens\n","\n","    def convert_record(self, record):\n","\n","        messages = []\n","\n","        message_user = f\"Your task is to determine the acceptability of the text for the English language in terms of syntax, morphology and semantics. The answer should be one number: 0 or 1, where 0 means the sentence is not acceptable from the point of view of the English language, 1 means it is acceptable.\\nText:{record['sentence']}\"\n","        message_bot_train = f\"Answer: {record['label']}\"\n","\n","        messages.append({'role': 'user', 'content': message_user})\n","        messages.append({'role': 'bot', 'content': message_bot_train})\n","        # к сообщению применяется токенизатор и устанавливается чат темплейт\n","        input_ids = self.get_tokens(messages)\n","        labels = input_ids\n","\n","        # проверка на макс. длинну чата, установленную моделью\n","        if len(input_ids) > self.max_tokens_count - 2:\n","            return None\n","        # в этом блоке устанавливается внимание на весь чат/на ответ модели\n","        labels_mask = [\n","            self.labels_pad_token_id for _ in range(len(input_ids))\n","        ]\n","        if (\n","            self.only_target_loss\n","        ):\n","            message_labels = labels_mask\n","\n","\n","        if not input_ids:\n","            return None\n","\n","        # не совсем понятно зачем это здесь\n","        # original_input_ids = self.get_tokens(record[\"sentence\"])\n","        # if input_ids != original_input_ids[: len(input_ids)]:\n","        #     print(input_ids)\n","        #     print(original_input_ids[: len(input_ids)])\n","        # assert input_ids == original_input_ids[: len(input_ids)]\n","\n","        # добавляем глабальный bos\n","        if self.add_global_bos and input_ids[0] != self.tokenizer.bos_token_id:\n","            input_ids.insert(0, self.tokenizer.bos_token_id)\n","            labels.insert(0, self.labels_pad_token_id)\n","\n","        # обрезаем спецсивол после eos\n","        if input_ids[-2] == self.tokenizer.eos_token_id:\n","            input_ids = input_ids[:-1]\n","            labels = labels[:-1]\n","\n","        # добавляем глобальный eos\n","        if self.add_global_eos and input_ids[-1] != self.tokenizer.eos_token_id:\n","            input_ids.append(self.tokenizer.eos_token_id)\n","            labels.append(self.tokenizer.eos_token_id)\n","\n","        # выводим 1 из сообщений в датасете для сверки содержимого чат датасета\n","        if not self.is_printed:\n","            print(input_ids)\n","            print(labels)\n","            print(\n","                \"Full prompt:\" +\n","                self.tokenizer.decode(input_ids, skip_special_tokens=False)\n","            )\n","            assert '\\n' in self.tokenizer.decode(input_ids, skip_special_tokens=False)\n","            self.is_printed = True\n","\n","\n","        input_ids = torch.LongTensor(input_ids)\n","        labels = torch.LongTensor(labels)\n","        attention_mask = input_ids.new_ones(input_ids.size())\n","        assert (\n","            input_ids.size(0)\n","            == labels.size(0)\n","            == attention_mask.size(0)\n","            <= self.max_tokens_count\n","        )\n","        return {\n","            \"input_ids\": input_ids,\n","            \"labels\": labels,\n","            \"attention_mask\": attention_mask,\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N1UulUnL8GW9"},"outputs":[],"source":["from datasets import load_dataset\n","dataset = load_dataset('nyu-mll/glue','cola')\n","dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QwBC07pwrExf"},"outputs":[],"source":["dataset = dataset['train'].select(range(1000))\n","dataset = dataset.train_test_split(test_size=0.1)\n","dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mawxTB-n9oGz"},"outputs":[],"source":["os.environ[\"WANDB_DISABLED\"] = \"true\"\n","# отключаем сервис WandB что бы он не собирал статистику"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w-Xda5PL942A"},"outputs":[],"source":["# используем чат темплейт от Ruadapt версии так как он не прокидывает system часть в промпт\n","tokenizer = AutoTokenizer.from_pretrained('RefalMachine/RuadaptQwen2.5-7B-Lite-Beta')\n","chat_template = tokenizer.chat_template\n","chat_template"]},{"cell_type":"code","source":["model_name = 'Qwen/Qwen2.5-1.5B-Instruct'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"],"metadata":{"id":"cDg5-wmRwo8H"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J1z4UZt399mN"},"outputs":[],"source":["# загружаем инструктивную модель unsloth\n","max_tokens_count = 1024\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name=model_name,\n","    max_seq_length=max_tokens_count,\n","    dtype=torch.float16,\n","    load_in_4bit=True,\n","    attn_implementation=\"sdpa\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VXuHgw7j_Zaw"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.chat_template = chat_template\n","tokenizer.padding_side = 'left'\n","# устанавливаем токены и чат темплейт"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8WwVg96_dI9"},"outputs":[],"source":["only_target_loss = True\n","\n","datasets = []\n","for split in ('train','test'):\n","    datasets.append(\n","        ChatDataset(\n","            dataset[split],\n","            tokenizer,\n","            max_tokens_count=max_tokens_count,\n","            sample_rate=1.0,\n","            only_target_loss=only_target_loss,\n","            add_global_eos=False,\n","            add_global_bos=False\n","        )\n","    )\n","train_dataset, val_dataset = datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O0oUAShK_0Ft"},"outputs":[],"source":["from transformers import GenerationConfig\n","\n","# def generate(messages, model, tokenizer, generation_config):\n","#     print(tokenizer.apply_chat_template(messages, add_special_tokens=False, add_generation_prompt=True, tokenize=False))\n","#     input_ids = tokenizer.apply_chat_template(messages, return_tensors='pt', add_special_tokens=False, add_generation_prompt=True)\n","#     input_ids = input_ids.to(model.device)\n","#     with torch.no_grad():\n","#         output_ids = model.generate(\n","#             input_ids,\n","#             generation_config=generation_config\n","#         )\n","#     outputs = []\n","#     for sample_output_ids, sample_input_ids in zip(output_ids, input_ids):\n","#         sample_output_ids = sample_output_ids[len(sample_input_ids):]\n","#         sample_output = tokenizer.decode(sample_output_ids, skip_special_tokens=True)\n","#         outputs.append(sample_output)\n","\n","#     if len(outputs) == 1:\n","#         outputs = outputs[0]\n","#     return outputs\n","\n","\n","\n","generation_config = GenerationConfig.from_dict(\n","    {\n","        'top_k': 20,\n","        'top_p': 0.8,\n","        'temperature': 0.1,\n","        'repetition_penalty': 1.0,\n","        'max_new_tokens': 64,\n","        'do_sample': True,\n","        'pad_token_id': tokenizer.pad_token_id,\n","        'bos_token_id': tokenizer.bos_token_id,\n","        'eos_token_id': tokenizer.eos_token_id\n","    }\n",")\n","generation_config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"75caqlan_4qX"},"outputs":[],"source":["lora_config = {\n","    \"r\": 32,\n","    \"lora_alpha\": 16,\n","    \"lora_dropout\": 0.0,\n","    \"bias\": \"none\",\n","    \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n","    \"use_gradient_checkpointing\": \"unsloth\"\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WYPXTJnA_6rw"},"outputs":[],"source":["model = FastLanguageModel.get_peft_model(\n","    model, **lora_config, max_seq_length=max_tokens_count\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJ_m-CstADXu"},"outputs":[],"source":["training_args = {\n","    \"per_device_train_batch_size\": 1,\n","    \"per_device_eval_batch_size\": 1,\n","    \"gradient_accumulation_steps\": 8,\n","    \"eval_steps\": 16,\n","    \"save_steps\": 128,\n","    \"logging_steps\": 16,\n","    \"learning_rate\": 0.00005,\n","    \"num_train_epochs\": 1,\n","    \"lr_scheduler_type\": \"cosine\",\n","    \"warmup_steps\": 16,\n","    \"bf16\": False,\n","    \"fp16\": True,\n","    \"optim\": \"paged_adamw_8bit\",\n","    \"save_total_limit\": 1,\n","    \"seed\": 1337,\n","    \"max_grad_norm\": 1.0,\n","    \"weight_decay\": 0.05\n","}\n","training_args = UnslothTrainingArguments(output_dir='./instruct_unsloth', **training_args)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kp6a82q6AGw3"},"outputs":[],"source":["from unsloth.trainer import _create_unsloth_optimizer\n","class CustomTrainer(Trainer):\n","    def create_optimizer(self):\n","        embedding_learning_rate = getattr(self.args, \"embedding_learning_rate\", None)\n","        if embedding_learning_rate is None:\n","            return super().create_optimizer()\n","        if self.optimizer is None:\n","            optimizer_cls, optimizer_kwargs = Trainer.get_optimizer_cls_and_kwargs(self.args)\n","            self.optimizer = _create_unsloth_optimizer(\n","                self.model,\n","                optimizer_cls,\n","                optimizer_kwargs,\n","                embedding_learning_rate,\n","            )\n","        return self.optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GBqnasxbAitD"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1_Gyla_tAJV2"},"outputs":[],"source":["# Занимает в памяти всего 3.5GB для 1.5B модели при обучении!\n","trainer = data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8)\n","trainer = CustomTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    data_collator=data_collator,\n",")\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Se2Odj33Acvi"},"outputs":[],"source":["model.save_pretrained('./trained_qwen_model_lora')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QmVbFMt_AeXS"},"outputs":[],"source":["tokenizer.save_pretrained('./trained_qwen_model_lora')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jS407LD0Af6q"},"outputs":[],"source":["!ls trained_qwen_model_lora"]},{"cell_type":"markdown","source":["Обязательно перезапустить сеанс\n"],"metadata":{"id":"i0Z7SGdtR1Rs"}},{"cell_type":"code","source":["%cd llmtf_open"],"metadata":{"id":"-Yk-Wjybx3IF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from llmtf.model import HFModel\n","from typing import Dict, List, Tuple\n","from llmtf.metrics import mean\n","from llmtf.base import SimpleFewShotHFTask\n","from sklearn.metrics import matthews_corrcoef"],"metadata":{"id":"LnvPbkJYxcQF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u258KIEWbIiS"},"outputs":[],"source":["class GlueColaTask(SimpleFewShotHFTask):\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","        self.method = 'calculate_tokens_proba'\n","        self._max_new_tokens = 1\n","\n","    @classmethod\n","    def name(cls):\n","        return 'glue/cola'\n","\n","    @property\n","    def choices(self):\n","        return [\"0\", \"1\"]\n","\n","    def aggregation(self) -> Dict:\n","        return {\"acc\": mean, \"mcc\": lambda data: matthews_corrcoef([d[0] for d in data],[d[1] for d in data])}\n","\n","    def dataset_args(self) -> Dict:\n","        return {'path': '../glue/cola'}\n","\n","\n","    def evaluate(self, sample, y_pred) -> Dict:\n","        y_true = str(sample['label'])\n","        y_pred = sorted([pair for pair in y_pred.items()], key=lambda x: -x[1])[0][0]\n","        return {\"acc\": y_true == y_pred, \"mcc\": [y_true, y_pred]}\n","\n","    def test_split_name(self) -> str:\n","        return 'validation'\n","\n","    def prompt_split_name(self) -> str:\n","        return 'train'\n","\n","    def create_messages(self, sample, with_answer=False) -> List[Dict]:\n","\n","        messages = []\n","\n","        instruction_user = \"Your task is to determine the acceptability of the text for the English language in terms of syntax, morphology and semantics. The answer should be one number: 0 or 1, where 0 means the sentence is not acceptable from the point of view of the English language, 1 means it is acceptable.\\nText:{sentence}\"\n","        instruction_bot = \"Answer:\"\n","\n","        messages.append({'role': 'user', 'content': instruction_user.format(**sample)})\n","        messages.append({'role': 'bot', 'content': instruction_bot})\n","\n","        return messages"]},{"cell_type":"code","source":["task = GlueColaTask()"],"metadata":{"id":"sj_fxWjMNWLX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Xzh5X5_b7Vn"},"outputs":[],"source":["model_name = \"./trained_qwen_model_lora\"\n","model = HFModel(device_map=\"cuda\",attn_implementation=\"sdpa\")\n","model.from_pretrained(model_name)"]},{"cell_type":"code","source":["model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n","model = HFModel(device_map=\"cuda\",attn_implementation=\"sdpa\")\n","model.from_pretrained(model_name)"],"metadata":{"id":"knE5T3uz1yO0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from llmtf.evaluator import Evaluator\n","evaluator = Evaluator()\n","\n","evaluator.evaluate_dataset(\n","    task=task,\n","    model=model,\n","    output_dir='../cola-qwen-no_finetune',\n","    max_len=4000,\n","    few_shot_count=0,\n","    generation_config=None, # will use model.generation_config by default\n","    batch_size=4,\n","    max_sample_per_dataset=200\n",")"],"metadata":{"id":"3w_s6mb-SGm6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cat ../cola-qwen-clear/glue_cola_total.jsonl"],"metadata":{"id":"83_9y9R_TORi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cat ../cola-qwen-lora-16/glue_cola_total.jsonl"],"metadata":{"id":"fxSoI_QMTOaA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cat ../cola-qwen-lora-32/glue_cola_total.jsonl"],"metadata":{"id":"oTcZFpYYXiLC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zHwMy0XoXhno"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Эту проблему надо решить, я не понимаю, почему она возникает.\n","В качестве решения я закомментил assert. Метрики на обычной модели совпали с действительными."],"metadata":{"id":"VXzRnhJtSQus"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7sy0oMgqcKD8"},"outputs":[],"source":["from llmtf.evaluator import Evaluator\n","evaluator = Evaluator()\n","\n","evaluator.evaluate_dataset(\n","    task=task,\n","    model=model,\n","    output_dir='../cola-qwen-no_l',\n","    max_len=4000,\n","    few_shot_count=0,\n","    generation_config=None, # will use model.generation_config by default\n","    batch_size=4,\n","    max_sample_per_dataset=200\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyN9mzM5gdmQIjWp6M6hMu+p"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}